{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a839f56",
   "metadata": {},
   "source": [
    "- 이 문서는 아래의 주소의 pdf에서 내용을 발췌하여 만들어 졌습니다.\n",
    "- https://github.com/KPFBERT/kpfbert/blob/main/BERT-MediaNavi.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc808091",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14c14d",
   "metadata": {},
   "source": [
    "- BERT는 구글에서 만들었다.\n",
    "- 앞에 나온 단어로 다음에 올 단어를 예측하는 것이 아니라 문장의 중간 단어를 마스킹한 후 전체 문장에서 해당 단어를 예측하는 방식으로 학습된다. 이를 Masked Language Model(MLM)이라고 부른다.\n",
    "- 두 문장이 이어지는 관계인지 아닌지를 학습하는 기능도 가지고 있다. 이것을 Next Sentence Prediction(NSP)라 부른다.\n",
    "- BERT는 문장을 생성하지 않고 문장을 분석하고 이해하는데만 집중하는 모델로 Transformer 구조에서 디코더를 생략하고 인코더만 이용했다.\n",
    "- 많은 데이터를 학습하여 하이퍼 파라미터 값을 생성해 놓았더니, 각각의 독립적인 분류, 추론, 문장비교, 질문대답 등의 테스크에서 간단한 레이어를 추가하고, 적은 데이터와 학습시간으로 미세조정만 거쳐도 기존의 각 테스크별 SOTA 모델들을 압도하는 성능을 보여주었다. 이를 전이학습(Transfer Learning)이라 부른다.\n",
    "- 출시 당시 매우 거대한 모델이라 학습시간이 오래 걸렸다. 하지만 미리 학습된 기본모델과 소스코드를 구글에서 오픈소스로 공개하였기 때문에 자유롭게 가져다 쓸 수 있따."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6b360",
   "metadata": {},
   "source": [
    "## BERT의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa5fda",
   "metadata": {},
   "source": [
    "- 문장을 토큰화 해서 전체 문장벡터를 만든다. 문장 시작은 (CLS), 문장끝은 (SEP)이라는 특수한 토큰을 표시한다. 이외에도 몇가지 특수토큰이 존재한다.\n",
    "- 문장별로 구분하는 Segment Embeddings을 만든다. 첫번째 문장 0, 두번째 문장 1\n",
    "- 각 토큰의 위치를 표시하는 Position Embeddings를 만든다. 이는 Transformer의 positional embedding과 다르다.\n",
    "- 이 세가지를 합산하여 입력으로 전달된다.\n",
    "- 각 토큰에 해당하는 출력벡터가 출력된다.\n",
    "- 이후에 출력값에 적당한 레이어를 추가하여 값을 가공하여(fine-tuning) 원하는테스크에 적용하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c410843a",
   "metadata": {},
   "source": [
    "## BERT의 활용법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b65cfa",
   "metadata": {},
   "source": [
    "- 두 문장의 연관성파악, 단일문장의 분류문제, 태깅문제, 질문과 답 문제 등에 적용이 가능하다.\n",
    "- BERT 모델(레이어) 위에 추가적인 레이어를 올려 다른 새로운 태스크에도 적용해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf50fb",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f25ec",
   "metadata": {},
   "source": [
    "- OpenAI라는 단체에서 Transformer 구조에서 인코더는 무시하고 디코더 부분만 집중하여 문장생성모델을 만들었는데, 이것이 GPT이다.\n",
    "- 대량의 문서를 학습하여 어떤 단어가 주어졌을 때 다음에 올 확률이 가장 높은 단어를 제시하여 순차적으로 문장을 만들어 나간다.\n",
    "- GPT는 학습데이터가 많을수록 상당한 성능 향상을 발견하여 현재 버전3 에서는 인간이 작문하였다고 봐도 크게 이상하지 않을 정도의 문장생성 능력을 보여준다. 다만, 사용된 파라미터만 175B(1,750억)개로 BERT-base 모델 110M(1.1억)개의 1,600배에 달한다.\n",
    "- 참고로 OPEN-AI에서 GPT-1이 먼저 나오고 Google에서 더 성능좋은 BERT를 내 놓았고, OPEN-AI에서 GPT-2, 3로 업그레이드 하면서 서로 성능경쟁으로 발전하는 관계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa152c",
   "metadata": {},
   "source": [
    "# BART "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e784d",
   "metadata": {},
   "source": [
    "- BERT가 분류등의 일반 테스크 성능은 뛰어나지만, 문장생성 등의 테스크에서는 매우 취약함을 보이는데, 이는 디코더를 사용하지 않기 때문이다. 이를 보완하기 위해서BART는 BERT방식에, 뒤에 소개될 GPT의 디코더 구조를 사용하여 나머지 성능은 유지하고, 문장생성, 지문해석 등의 태스크에서 큰 성능향상을 보여준다.\n",
    "- 기본원리는 denoising autoencoder 방식으로 사전학습이 되는데, BERT의 MLM방식처럼 입력 텍스트에 일정부분 변형(noise)을 가하고, 이것을 원래대로 복구하는 방식으로 학습이 진행된다. BART의 특징은 BERT는 단어 하나를 masking 하였는데, BART에서는 어떤형태의 변형(noise)방법이든 적용 가능 하다는 것이다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
